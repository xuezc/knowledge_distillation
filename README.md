# knowledge_distillation

一．从类概率蒸馏
1.	QUEST: Quantized embedding space for transferring knowledge(CVPR 2020)：代替在教师网络的初始特征空间进行蒸馏操作，文章先将原始特征空间转化为一个对特征扰动更为鲁棒的量化空间再进行蒸馏。在该量化空间中，更关注重要的语义概念及其在知识蒸馏中的空间相关性。
2.	Ensemble Distribution Distillation(ICLR 2020)：文章将集合蒸馏和先验网络相结合，提出了一种新的集合分布蒸馏方法，将集合分布蒸馏成一个先验网络。这使得单个模型既能保留改进的分类性能，又能保留集合的多样性。
3.	Noisy Collaboration in Knowledge Distillation(ICLR 2020)：文章认为噪声是改进神经网络训练和解决明显矛盾的目标的一个关键因素，即可以提高模型的泛化性和鲁棒性。受多个噪声源引起的大脑变异性试验的启发，通过输入电平或监测信号的噪声来引入变异性。结果表明，噪声可以提高模型的泛化能力和鲁棒性。

二．从中间层蒸馏
1.	Feature-map-level Online Adversarial Knowledge Distillation(ICLR 2020)：文章提出一种在线蒸馏方法，该方法同时训练多个网络，并通过使用判别器来区分不同网络的特征图分布。其中，每个网络都有相应的判别器，该判别器在将另一个网络的特征图分类为真的同时，将特征图与自身的特征图区分为假。通过训练一个网络来欺骗相应的判别器，它可以学习另一个网络的特征图分布。此外，文章还提出了一种循环学习方法来训练两个以上的网络。文章将该方法应用到分类任务的各种网络结构中，发现在训练一对小网络和一对大网络的情况下，性能有显著的提高。
2.	Knowledge Squeezed Adversarial Network Compression(AAAI 2020)：受网络规模差距过大，小网络无法完美模拟大网络的假设启发，文章提出了一种在对抗性训练框架下学习学生网络的知识转移方法，包括有效的中间监督。为了实现功能强大和高度紧凑的中间信息表示，压缩的知识通过任务驱动的注意机制来实现。这样，教师网络的知识转移就可以适应学生网络的规模。结果表明，该方法综合了面向过程学习和面向结果学习的优点。
3.	Knowledge Distillation from Internal Representations(AAAI 2020)：文章指出了当教师规模相当大时，并不能保证教师的知识会转移到学生身上；即使学生与软标签紧密匹配，其内部表现也可能大不相同。这种内部的不匹配可能会破坏最初从教师转移到学生身上的泛化能力。文中除了使用kl散度之外，使用了余弦相似度损失用以转移中间层的暗知识。
4.	Structured Knowledge Distillation for Dense Prediction：文章将结构信息从大型网络传输到小型网络来进行密集预测任务，文章具体研究了两种结构化的蒸馏方案，一是通过建立静态图来提取成对相似度的成对蒸馏，另一个是使用对抗训练来提取整体知识的整体蒸馏。并通过对语义分割、深度估计和目标检测这三个密集预测任务的大量实验，来证明提出方法的有效性。
